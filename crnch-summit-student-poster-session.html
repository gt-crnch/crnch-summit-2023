<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>@font-face {
  font-family: octicons-link;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

body {
    width: 980px;
    margin-right: auto;
    margin-left: auto;
    color:#333;
    background:#fff;
}

body .markdown-body {
    padding: 45px;
    word-wrap: break-word;
}

.markdown-body .octicon-link:before {
  font: normal normal normal 16px/1 octicons-link;
  display: inline-block;
  text-decoration: none;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  content: '\f05c';
  vertical-align: middle;
}

.markdown-body .anchor {
  float: left;
  line-height: 1;
  margin-left: -20px;
  padding-right: 4px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #1b1f23;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #24292e;
  line-height: 1.5;
  font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol;
  font-size: 16px;
  line-height: 1.5;
  word-wrap: break-word;
}

.markdown-body .pl-c {
  color: #6a737d;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #005cc5;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #6f42c1;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #24292e;
}

.markdown-body .pl-ent {
  color: #22863a;
}

.markdown-body .pl-k {
  color: #d73a49;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #032f62;
}

.markdown-body .pl-smw,
.markdown-body .pl-v {
  color: #e36209;
}

.markdown-body .pl-bu {
  color: #b31d28;
}

.markdown-body .pl-ii {
  background-color: #b31d28;
  color: #fafbfc;
}

.markdown-body .pl-c2 {
  background-color: #d73a49;
  color: #fafbfc;
}

.markdown-body .pl-c2:before {
  content: "^M";
}

.markdown-body .pl-sr .pl-cce {
  color: #22863a;
  font-weight: 700;
}

.markdown-body .pl-ml {
  color: #735c0f;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #005cc5;
  font-weight: 700;
}

.markdown-body .pl-mi {
  color: #24292e;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #24292e;
  font-weight: 700;
}

.markdown-body .pl-md {
  background-color: #ffeef0;
  color: #b31d28;
}

.markdown-body .pl-mi1 {
  background-color: #f0fff4;
  color: #22863a;
}

.markdown-body .pl-mc {
  background-color: #ffebda;
  color: #e36209;
}

.markdown-body .pl-mi2 {
  background-color: #005cc5;
  color: #f6f8fa;
}

.markdown-body .pl-mdr {
  color: #6f42c1;
  font-weight: 700;
}

.markdown-body .pl-ba {
  color: #586069;
}

.markdown-body .pl-sg {
  color: #959da5;
}

.markdown-body .pl-corl {
  color: #032f62;
  text-decoration: underline;
}

.markdown-body details {
  display: block;
}

.markdown-body summary {
  display: list-item;
}

.markdown-body a {
  background-color: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline-width: 0;
}

.markdown-body strong {
  font-weight: inherit;
  font-weight: bolder;
}

.markdown-body h1 {
  font-size: 2em;
  margin: .67em 0;
}

.markdown-body img {
  border-style: none;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace,monospace;
  font-size: 1em;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
  overflow: visible;
}

.markdown-body input {
  font: inherit;
  margin: 0;
}

.markdown-body input {
  overflow: visible;
}

.markdown-body [type=checkbox] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body * {
  box-sizing: border-box;
}

.markdown-body input {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}

.markdown-body a {
  color: #0366d6;
  text-decoration: none;
}

.markdown-body a:hover {
  text-decoration: underline;
}

.markdown-body strong {
  font-weight: 600;
}

.markdown-body hr {
  background: transparent;
  border: 0;
  border-bottom: 1px solid #dfe2e5;
  height: 0;
  margin: 15px 0;
  overflow: hidden;
}

.markdown-body hr:before {
  content: "";
  display: table;
}

.markdown-body hr:after {
  clear: both;
  content: "";
  display: table;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body details summary {
  cursor: pointer;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-bottom: 0;
  margin-top: 0;
}

.markdown-body h1 {
  font-size: 32px;
}

.markdown-body h1,
.markdown-body h2 {
  font-weight: 600;
}

.markdown-body h2 {
  font-size: 24px;
}

.markdown-body h3 {
  font-size: 20px;
}

.markdown-body h3,
.markdown-body h4 {
  font-weight: 600;
}

.markdown-body h4 {
  font-size: 16px;
}

.markdown-body h5 {
  font-size: 14px;
}

.markdown-body h5,
.markdown-body h6 {
  font-weight: 600;
}

.markdown-body h6 {
  font-size: 12px;
}

.markdown-body p {
  margin-bottom: 10px;
  margin-top: 0;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ol,
.markdown-body ul {
  margin-bottom: 0;
  margin-top: 0;
  padding-left: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ol ol ol,
.markdown-body ol ul ol,
.markdown-body ul ol ol,
.markdown-body ul ul ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre {
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-bottom: 0;
  margin-top: 0;
}

.markdown-body input::-webkit-inner-spin-button,
.markdown-body input::-webkit-outer-spin-button {
  -webkit-appearance: none;
  appearance: none;
  margin: 0;
}

.markdown-body .border {
  border: 1px solid #e1e4e8!important;
}

.markdown-body .border-0 {
  border: 0!important;
}

.markdown-body .border-bottom {
  border-bottom: 1px solid #e1e4e8!important;
}

.markdown-body .rounded-1 {
  border-radius: 3px!important;
}

.markdown-body .bg-white {
  background-color: #fff!important;
}

.markdown-body .bg-gray-light {
  background-color: #fafbfc!important;
}

.markdown-body .text-gray-light {
  color: #6a737d!important;
}

.markdown-body .mb-0 {
  margin-bottom: 0!important;
}

.markdown-body .my-2 {
  margin-bottom: 8px!important;
  margin-top: 8px!important;
}

.markdown-body .pl-0 {
  padding-left: 0!important;
}

.markdown-body .py-0 {
  padding-bottom: 0!important;
  padding-top: 0!important;
}

.markdown-body .pl-1 {
  padding-left: 4px!important;
}

.markdown-body .pl-2 {
  padding-left: 8px!important;
}

.markdown-body .py-2 {
  padding-bottom: 8px!important;
  padding-top: 8px!important;
}

.markdown-body .pl-3,
.markdown-body .px-3 {
  padding-left: 16px!important;
}

.markdown-body .px-3 {
  padding-right: 16px!important;
}

.markdown-body .pl-4 {
  padding-left: 24px!important;
}

.markdown-body .pl-5 {
  padding-left: 32px!important;
}

.markdown-body .pl-6 {
  padding-left: 40px!important;
}

.markdown-body .f6 {
  font-size: 12px!important;
}

.markdown-body .lh-condensed {
  line-height: 1.25!important;
}

.markdown-body .text-bold {
  font-weight: 600!important;
}

.markdown-body:before {
  content: "";
  display: table;
}

.markdown-body:after {
  clear: both;
  content: "";
  display: table;
}

.markdown-body>:first-child {
  margin-top: 0!important;
}

.markdown-body>:last-child {
  margin-bottom: 0!important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body blockquote,
.markdown-body dl,
.markdown-body ol,
.markdown-body p,
.markdown-body pre,
.markdown-body table,
.markdown-body ul {
  margin-bottom: 16px;
  margin-top: 0;
}

.markdown-body hr {
  background-color: #e1e4e8;
  border: 0;
  height: .25em;
  margin: 24px 0;
  padding: 0;
}

.markdown-body blockquote {
  border-left: .25em solid #dfe2e5;
  color: #6a737d;
  padding: 0 1em;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #fafbfc;
  border: 1px solid #c6cbd1;
  border-bottom-color: #959da5;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #959da5;
  color: #444d56;
  display: inline-block;
  font-size: 11px;
  line-height: 10px;
  padding: 3px 5px;
  vertical-align: middle;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  font-weight: 600;
  line-height: 1.25;
  margin-bottom: 16px;
  margin-top: 24px;
}

.markdown-body h1 {
  font-size: 2em;
}

.markdown-body h1,
.markdown-body h2 {
  border-bottom: 1px solid #eaecef;
  padding-bottom: .3em;
}

.markdown-body h2 {
  font-size: 1.5em;
}

.markdown-body h3 {
  font-size: 1.25em;
}

.markdown-body h4 {
  font-size: 1em;
}

.markdown-body h5 {
  font-size: .875em;
}

.markdown-body h6 {
  color: #6a737d;
  font-size: .85em;
}

.markdown-body ol,
.markdown-body ul {
  padding-left: 2em;
}

.markdown-body ol ol,
.markdown-body ol ul,
.markdown-body ul ol,
.markdown-body ul ul {
  margin-bottom: 0;
  margin-top: 0;
}

.markdown-body li {
  word-wrap: break-all;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body li+li {
  margin-top: .25em;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  font-size: 1em;
  font-style: italic;
  font-weight: 600;
  margin-top: 16px;
  padding: 0;
}

.markdown-body dl dd {
  margin-bottom: 16px;
  padding: 0 16px;
}

.markdown-body table {
  display: block;
  overflow: auto;
  width: 100%;
}

.markdown-body table th {
  font-weight: 600;
}

.markdown-body table td,
.markdown-body table th {
  border: 1px solid #dfe2e5;
  padding: 6px 13px;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #c6cbd1;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f6f8fa;
}

.markdown-body img {
  background-color: #fff;
  box-sizing: content-box;
  max-width: 100%;
}

.markdown-body img[align=right] {
  padding-left: 20px;
}

.markdown-body img[align=left] {
  padding-right: 20px;
}

.markdown-body code {
  background-color: rgba(27,31,35,.05);
  border-radius: 3px;
  font-size: 85%;
  margin: 0;
  padding: .2em .4em;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre>code {
  background: transparent;
  border: 0;
  font-size: 100%;
  margin: 0;
  padding: 0;
  white-space: pre;
  word-break: normal;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body .highlight pre,
.markdown-body pre {
  background-color: #f6f8fa;
  border-radius: 3px;
  font-size: 85%;
  line-height: 1.45;
  overflow: auto;
  padding: 16px;
}

.markdown-body pre code {
  background-color: transparent;
  border: 0;
  display: inline;
  line-height: inherit;
  margin: 0;
  max-width: auto;
  overflow: visible;
  padding: 0;
  word-wrap: normal;
}

.markdown-body .commit-tease-sha {
  color: #444d56;
  display: inline-block;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 90%;
}

.markdown-body .blob-wrapper {
  border-bottom-left-radius: 3px;
  border-bottom-right-radius: 3px;
  overflow-x: auto;
  overflow-y: hidden;
}

.markdown-body .blob-wrapper-embedded {
  max-height: 240px;
  overflow-y: auto;
}

.markdown-body .blob-num {
  -moz-user-select: none;
  -ms-user-select: none;
  -webkit-user-select: none;
  color: rgba(27,31,35,.3);
  cursor: pointer;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 12px;
  line-height: 20px;
  min-width: 50px;
  padding-left: 10px;
  padding-right: 10px;
  text-align: right;
  user-select: none;
  vertical-align: top;
  white-space: nowrap;
  width: 1%;
}

.markdown-body .blob-num:hover {
  color: rgba(27,31,35,.6);
}

.markdown-body .blob-num:before {
  content: attr(data-line-number);
}

.markdown-body .blob-code {
  line-height: 20px;
  padding-left: 10px;
  padding-right: 10px;
  position: relative;
  vertical-align: top;
}

.markdown-body .blob-code-inner {
  color: #24292e;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 12px;
  overflow: visible;
  white-space: pre;
  word-wrap: normal;
}

.markdown-body .pl-token.active,
.markdown-body .pl-token:hover {
  background: #ffea7f;
  cursor: pointer;
}

.markdown-body kbd {
  background-color: #fafbfc;
  border: 1px solid #d1d5da;
  border-bottom-color: #c6cbd1;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #c6cbd1;
  color: #444d56;
  display: inline-block;
  font: 11px SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  line-height: 10px;
  padding: 3px 5px;
  vertical-align: middle;
}

.markdown-body :checked+.radio-label {
  border-color: #0366d6;
  position: relative;
  z-index: 1;
}

.markdown-body .tab-size[data-tab-size="1"] {
  -moz-tab-size: 1;
  tab-size: 1;
}

.markdown-body .tab-size[data-tab-size="2"] {
  -moz-tab-size: 2;
  tab-size: 2;
}

.markdown-body .tab-size[data-tab-size="3"] {
  -moz-tab-size: 3;
  tab-size: 3;
}

.markdown-body .tab-size[data-tab-size="4"] {
  -moz-tab-size: 4;
  tab-size: 4;
}

.markdown-body .tab-size[data-tab-size="5"] {
  -moz-tab-size: 5;
  tab-size: 5;
}

.markdown-body .tab-size[data-tab-size="6"] {
  -moz-tab-size: 6;
  tab-size: 6;
}

.markdown-body .tab-size[data-tab-size="7"] {
  -moz-tab-size: 7;
  tab-size: 7;
}

.markdown-body .tab-size[data-tab-size="8"] {
  -moz-tab-size: 8;
  tab-size: 8;
}

.markdown-body .tab-size[data-tab-size="9"] {
  -moz-tab-size: 9;
  tab-size: 9;
}

.markdown-body .tab-size[data-tab-size="10"] {
  -moz-tab-size: 10;
  tab-size: 10;
}

.markdown-body .tab-size[data-tab-size="11"] {
  -moz-tab-size: 11;
  tab-size: 11;
}

.markdown-body .tab-size[data-tab-size="12"] {
  -moz-tab-size: 12;
  tab-size: 12;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 .2em .25em -1.6em;
  vertical-align: middle;
}

.markdown-body hr {
  border-bottom-color: #eee;
}

.markdown-body .pl-0 {
  padding-left: 0!important;
}

.markdown-body .pl-1 {
  padding-left: 4px!important;
}

.markdown-body .pl-2 {
  padding-left: 8px!important;
}

.markdown-body .pl-3 {
  padding-left: 16px!important;
}

.markdown-body .pl-4 {
  padding-left: 24px!important;
}

.markdown-body .pl-5 {
  padding-left: 32px!important;
}

.markdown-body .pl-6 {
  padding-left: 40px!important;
}

.markdown-body .pl-7 {
  padding-left: 48px!important;
}

.markdown-body .pl-8 {
  padding-left: 64px!important;
}

.markdown-body .pl-9 {
  padding-left: 80px!important;
}

.markdown-body .pl-10 {
  padding-left: 96px!important;
}

.markdown-body .pl-11 {
  padding-left: 112px!important;
}

.markdown-body .pl-12 {
  padding-left: 128px!important;
}
</style><title>crnch-summit-student-poster-session</title></head><body><article class="markdown-body"><h1><a id="user-content-crnch-summit-poster-session" class="anchor" aria-hidden="true" href="#crnch-summit-poster-session"><span aria-hidden="true" class="octicon octicon-link"></span></a>CRNCH Summit Poster Session</h1>
<p>The CRNCH Student Poster Session will be held on Feburary 2nd, 2023 from ~12 to 1:15 PM. We are excited to have a large number of posters from our CRNCH students, and we also are thrilled to be able to feature the work of some of our <a href="https://crnch.gatech.edu/content/crnch-fellowship" rel="nofollow">CRNCH PhD Fellowship winners</a>.</p>
<p>We note that in some cases, we have not linked posters due to work being under submission or at the presenter's request.</p>
<h3><a id="user-content-2022-2023-fellowship-winners" class="anchor" aria-hidden="true" href="#2022-2023-fellowship-winners"><span aria-hidden="true" class="octicon octicon-link"></span></a>2022-2023 Fellowship Winners</h3>
<table>
<thead>
<tr>
<th>Student Presenter, Student Authors</th>
<th>Poster Title</th>
<th>Advisor(s)</th>
<th>GT Department</th>
<th>[Poster] [Abstract]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Albert Cho, Anish Saxena, Srikar Vanavasam</td>
<td>"Use CXL, not DDR, for Scalable Server Processors"</td>
<td>Alexandros Daglis, Moinuddin Qureshi</td>
<td>ECE</td>
<td>
<a href="https://github.com/gt-crnch/crnch-summit-2023/blob/7308c9db812206e13aeca9be1a749b2c6fa9dc14/student_poster_session/posters/Albert_Cho_et_al_CRNCH_Summit_2023_Fellowship_Poster.pdf">Poster</a> <a href="#yz">Abstract</a>
</td>
</tr>
<tr>
<td>Afolabi Ige, Jennifer Hasler</td>
<td>"Synthesizing Analog Computing ASICs with Programmable Standard Cells"</td>
<td>Jennifer Hasler</td>
<td>ECE</td>
<td><a href="#ai">Abstract</a></td>
</tr>
<tr>
<td>Rishov Sarkar, Cong (Callie) Hao</td>
<td>"LightningSim: Fast and Accurate Trace-Based Simulation for HLS"</td>
<td>Cong (Callie) Hao</td>
<td>ECE</td>
<td><a href="#rs">Abstract</a></td>
</tr>
<tr>
<td>Zhixin (Jack) Song, Bryan Gard, Spencer Bryngelson</td>
<td>"Solving Partial Differential Equations on Noisy Quantum Computers"</td>
<td>Dr. Spencer H. Bryngelson</td>
<td>ECE</td>
<td>
<a href="https://github.com/gt-crnch/crnch-summit-2023/blob/7308c9db812206e13aeca9be1a749b2c6fa9dc14/student_poster_session/posters/Zhixin_Song_et_al_PDE_NISQ_CRNCH_Summit_2023_Poster.pdf">Poster</a> <a href="#zjs">Abstract</a>
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Student Presenter, Student Authors</th>
<th>Poster Title</th>
<th>Advisor(s)</th>
<th>GT Department</th>
<th>[Poster] [Abstract]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Elton Pinto, Austin Adams</td>
<td>"Neko: A quantum map-filter-reduce programming language"</td>
<td>Vivek Sarkar, Tom Conte, Jeffrey Young</td>
<td>ECE</td>
<td>
<a href="https://github.com/gt-crnch/crnch-summit-2023/blob/72ffdfb85055c7226eb916fd023028e60c096c60/student_poster_session/posters/Elton_Pinto_et_al_Neko_CRNCH_Summit_2023_Poster.pdf">Poster</a> <a href="#ep">Abstract</a>
</td>
</tr>
<tr>
<td>Ruobing Han, Jaewon Lee, Jun Chen, Bhanu Garg, Jeffrey Young, Mark Ahn, Xuele Zhou, John Lu, Haotian Sheng, Blaise Tine, Jaewoong Sim, Hyesoon Kim</td>
<td>"CuPBoP: CUDA for Parallelized and Broad-range Processors"</td>
<td>Hyesoon Kim</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#rh">Abstract</a>
</td>
</tr>
<tr>
<td>Pranav O. Mathews, Jennifer O. Hasler</td>
<td>"Physical Computing for Hopfield Networks on a Reconfigurable Analog IC"</td>
<td>Dr. Jennifer Hasler</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#pom">Abstract</a>
</td>
</tr>
<tr>
<td>Linhao Yang, Afolabi Ige, Jennifer Hasler, Hang Yang, Callie Hao</td>
<td>"Analog System High-level Synthesis to Physical Devices"</td>
<td>Jennifer Hasler, Callie Hao</td>
<td>ECE</td>
<td><a href="#ly">Abstract</a></td>
</tr>
<tr>
<td>Chaojian Li, Sixu Li, Yang (Katie) Zhao, Wenbo Zhu, Yingyan (Celine) Lin</td>
<td>"RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive AR/VR Rendering"</td>
<td>Yingyan (Celine) Lin</td>
<td>ECE</td>
<td>
<a href="https://github.com/gt-crnch/crnch-summit-2023/blob/72ffdfb85055c7226eb916fd023028e60c096c60/student_poster_session/posters/Chaojian_Lin_et_al_RT_NeRF_CRNCH_Summit_2023_Poster.pdf">Poster</a> <a href="#cl">Abstract</a>
</td>
</tr>
<tr>
<td>Cheng Wan, Yang (Katie) Zhao, Yingyan (Celine) Lin</td>
<td>"MixGCN: Scalable GCN Training by Integrating Mixtures of Parallelism, Accelerators, and Experts Models"</td>
<td>Yingyan (Celine) Lin</td>
<td>ECE</td>
<td><a href="#cw">Abstract</a></td>
</tr>
<tr>
<td>Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue Wang, Yingyan (Celine) Lin</td>
<td>"HW-NAS-Bench: Hardware-aware Neural Architecture Search Benchmark"</td>
<td>Yingyan (Celine) Lin</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#clzy">Abstract</a>
</td>
</tr>
<tr>
<td>Patrick Lavin, Sudhanshu Agarwal, Ryan Lunch, Jeffrey Young, Richard Vuduc</td>
<td>"Multifidelity DRAM Simulation in SST"</td>
<td>Jeffrey Young, Richard Vuduc</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#pl">Abstract</a>
</td>
</tr>
<tr>
<td>Mikhail Isaev, Nic McDonald, Jeff Young, Rich Vuduc</td>
<td>"ParaGraph: An application-simulator interface and toolkit for hardware-software co-design"</td>
<td>Richard Vuduc</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#mi">Abstract</a>
</td>
</tr>
<tr>
<td>Yonggan Fu, Yang Zhao, Yonggan Fu, Qixuan Yu, Yonggan Fu, Chaojian Li, Yonggan Fu, Yingyan (Celine) Lin</td>
<td>"2-in-1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency"</td>
<td>Yingyan (Celine) Lin</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#yf">Abstract</a>
</td>
</tr>
<tr>
<td>Haoran You, Cheng Wan, Yang Zhao, Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Shang Wu, Shunyao Zhang, Yongan Zhang, Chaojian Li, Vivek Boominathan, Ashok Veeraraghavan, Ziyun Li, Yingyan Lin</td>
<td>"EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm &amp; Accelerator Co-Design"</td>
<td>Yingyan (Celine) Lin</td>
<td>CS</td>
<td>
<a href="https://github.com/gt-crnch/crnch-summit-2023/blob/72ffdfb85055c7226eb916fd023028e60c096c60/student_poster_session/posters/Haoran_Yu_et_al_EyeCoD_CRNCH_Summit_2023_Poster.pdf">Poster</a> <a href="#hy">Abstract</a>
</td>
</tr>
<tr>
<td>Yongan Zhang, Haoran You, Yonggan Fu, Tong Geng, Ang Li, Yingyan Lin</td>
<td>"G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency"</td>
<td>Yingyan (Celine) Lin</td>
<td>CS</td>
<td>
<a href="">Poster</a> <a href="#yz">Abstract</a>
</td>
</tr>
<tr>
<td>Francisco Munoz Martinez, Raveesh Garg, José L. Abellán, Manuel E Acacio, Clay Hughes, Siva Rajamanickam, Tushar Krishna</td>
<td>"Cycle Accurate Simulation of AI Applications using STONNE, SST-STONNE and OMEGA"</td>
<td>Tushar Krishna</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#fmm">Abstract</a>
</td>
</tr>
<tr>
<td>Pulkit Gupta, Cynthia Wang, Shunzhi Wen, Tom Conte</td>
<td>"A High ILP Architecture for Supercunducting Electronics (SCE)"</td>
<td>Tom Conte</td>
<td>ECE</td>
<td><a href="#pgcw">Abstract</a></td>
</tr>
<tr>
<td>Saeed Rashidi, William Won, Sudarshan Srinivasan, Srinivas Sridharan, Tushar Krishna</td>
<td>"Themis: A Network Bandwidth-Aware Collective Scheduling Policy for Distributed Training of DL Models"</td>
<td>Tushar Krishna</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#srww">Abstract</a>
</td>
</tr>
<tr>
<td>Vima Gupta, Austin Adams, Elton Pinto, Dr. Jeffrey Young, Dr. Tom M Conte</td>
<td>"Effective qubit mapping, routing and scheduling for Ion-Shuttling Quantum Architectures"</td>
<td>Dr. Jeffrey Young, Dr. Tom Conte</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#vgaa">Abstract</a>
</td>
</tr>
<tr>
<td>Ryan Lynch, Austin Adams, Tom Conte, Jeff Young</td>
<td>"Leveraging MLIR to Augment a Python Quantum DSL"</td>
<td>Tom Conte, Jeff Young</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#rlaa">Abstract</a>
</td>
</tr>
<tr>
<td>Anirudh Jain, Pulkit Gupta, Tom Conte</td>
<td>"Residue Matrices for accelerating Sparse Kernels &amp; GraphBLAS"</td>
<td>Tom Conte</td>
<td>ECE</td>
<td><a href="#ajpg">Abstract</a></td>
</tr>
<tr>
<td>Payman Behnam, Jianming Tong, Alind Khare, Tushar Krishna, Alexey Tumanov</td>
<td>"SUSHI: SubGraph Stationary HW-SW Co-design for ML Inference"</td>
<td>Tushar Krishna, Alexey Tumanov</td>
<td>ECE</td>
<td>
<a href="">Poster</a> <a href="#pbjt">Abstract</a>
</td>
</tr>
<tr>
<td>Blaise Tine, Ruobing Han, Fares Elsabbagh, Krishna Praveen, Apurve Chawda, Will Gulian, Yaotian Feng, Da Eun Shim, Priyadarshini Roshan, Ethan Lyons, Varun Saxena, Santosh Srivatsan, Joshua R. Simpson, Fadi Alzammar, Liam Cooper, Sam Jijina, Swetha Rajagoplan, Tejaswini Anand Kumar, Jeff Young, Hyesoon Kim</td>
<td>"Vortex: Open GPU Research Platform"</td>
<td>Jeffrey Young, Hyesoon Kim</td>
<td>ECE</td>
<td>
<a href="https://github.com/gt-crnch/crnch-summit-2023/blob/72ffdfb85055c7226eb916fd023028e60c096c60/student_poster_session/posters/Liam_Cooper_Blaise_Tine_et_al_Vortex_CRNCH_Summit_2023_Poster.pdf">Poster</a> <a href="#btrh">Abstract</a>
</td>
</tr>
</tbody>
</table>
<h2><a id="user-content-student-abstracts" class="anchor" aria-hidden="true" href="#student-abstracts"><span aria-hidden="true" class="octicon octicon-link"></span></a>Student Abstracts:</h2>
<p><a id="user-content-ep"><strong>Elton Pinto, Austin Adams - "Neko: A quantum map-filter-reduce programming language"</strong></a></p>
<p>Programming quantum computers is hard. One has to painstakingly write code that builds a circuit using low-level quantum gates. In a way, writing a quantum program is analogous to writing assembly: it is tedious, error-prone, and hard to debug. The gate-level abstraction, albeit universal, is non-intuitive and too primitive to be used for rapidly prototyping large-scale quantum applications. There is a need to develop high-level abstractions that enable programmers to productively leverage the idiosyncrasies of quantum computing: quantum parallelism, interference, and entanglement.</p>
<p>In this ongoing work, I present Neko, a high-level quantum programming language that exposes a map-filter-reduce interface for exploiting quantum parallelism through the notion of first-class superpositions.</p>
<p><a id="user-content-rh"><strong>Ruobing Han, Jaewon Lee, Jun Chen, Bhanu Garg, Jeffrey Young, Mark Ahn, Xuele Zhou, John Lu, Haotian Sheng, Blaise Tine, Jaewoong Sim, Hyesoon Kim - "CuPBoP: CUDA for Parallelized and Broad-range Processors"</strong></a></p>
<p>CuPBoP is an open source framework, which supports executing CUDA on non-NVIDIA devices. Currently, we are working on supporting CUDA on CPUs, Vortex GPUs, AMD GPUs, and Intel GPUs. By supporting CUDA on non-NVIDIA devices, we can support Single-Kernel-Multiple-Device execution on heterogeneous systems.</p>
<p><a id="user-content-pom"><strong>Pranav O. Mathews, Jennifer O. Hasler - "Physical Computing for Hopfield Networks on a Reconfigurable Analog IC"</strong></a></p>
<p>This poster presents a physical computing framework to solve optimization problems using a Hopfield network built on a large scale Field Programmable Analog Array (FPAA). A core Hopfield circuit is presented that uses a programmable Vector-Maxtrix-Multiply (VMM) and OTA. The circuit dynamics of the VMM and effects of mismatch are discussed. The analog Hopfield network is evaluated by inputting a graph to the network and solving the NP-hard max-cut problem. Experimental results show convergence time in the order of microseconds towards a optimal solution on a four and ten node graph.</p>
<p><a id="user-content-ly"><strong>Linhao Yang, Afolabi Ige, Jennifer Hasler, Hang Yang, Callie Hao - "Analog System High-level Synthesis to Physical Devices"</strong></a></p>
<p>Designing analog systems requires significant labor and expertise due to the lack of automation tools to enable highly energy efficient, high-performance analog computing. This work involves the first automated analog synthesis tool from a very high-level representation to physical devices. The high level presentation can be either text-based (Python) or graphical (XCOS), and will be lowered to FPAA targeting or IC layouts.</p>
<p><a id="user-content-cl"><strong>Chaojian Li, Sixu Li, Yang (Katie) Zhao, Wenbo Zhu, Yingyan (Celine) Lin - "RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive AR/VR Rendering"</strong></a></p>
<p>Neural Radiance Field (NeRF) based rendering has attracted growing attention thanks to its state-of-the-art (SOTA) rendering quality and wide applications in Augmented and Virtual Reality (AR/VR). However, immersive real-time (&gt; 30 FPS) NeRF based rendering enabled interactions are still limited due to the low achievable throughput on AR/VR devices. To this end, we first profile SOTA efficient NeRF algorithms on commercial devices and identify two primary causes of the aforementioned inefficiency: (1) the uniform point sampling and (2) the dense accesses and computations of the required embeddings in NeRF. Furthermore, we propose RT-NeRF, which to the best of our knowledge is the first algorithm-hardware co-design acceleration of NeRF. Specifically, on the algorithm level, RT-NeRF integrates an efficient rendering pipeline for largely alleviating the inefficiency due to the commonly adopted uniform point sampling method in NeRF by directly computing the geometry of pre-existing points. Additionally, RT-NeRF leverages a coarse-grained view-dependent computing ordering scheme for eliminating the (unnecessary) processing of invisible points. On the hardware level, our proposed RT-NeRF accelerator (1) adopts a hybrid encoding scheme to adaptively switch between a bitmap- or coordinate-based sparsity encoding format for NeRF’s sparse embeddings, aiming to maximize the storage savings and thus reduce the required DRAM accesses while supporting efficient NeRF decoding; and (2) integrates both a high-density sparse search unit and a dual-purpose bi-direction adder &amp; search tree to coordinate the two aforementioned encoding formats. Extensive experiments on eight datasets consistently validate the effectiveness of RT-NeRF, achieving a large throughput improvement (e.g., 9.7×∼3,201×) while maintaining the rendering quality as compared with SOTA efficient NeRF solutions.</p>
<p><a id="user-content-cw"><strong>Cheng Wan, Yang (Katie) Zhao, Yingyan (Celine) Lin - "MixGCN: Scalable GCN Training by Integrating Mixtures of Parallelism, Accelerators, and Experts Models"</strong></a></p>
<p>Graph convolutional networks (GCNs) have demonstrated superiority on graph-based learning tasks. However, training GCNs on large graphs is particularly challenging, due to the following challenges: (1) the associated feature tensors can easily explode the memory and block the communication bandwidth of modern accelerators, and (2) the computation workflow in training GCNs alternates between sparse and dense matrix operations. Despite promising progress, existing solutions for scalable distributed GCN training mostly adopt partition parallelism, which is still unsatisfactory as they only partially address the first challenge while incurring scaled-out memory overhead and communication volume. To this end, we propose MIXGCN aiming to simultaneously address both the aforementioned challenges towards scalable GCN training. Specifically, on the system level, MIXGCN integrates a mixture of parallelism, for which both theoretical and empirical analysis verify its reduced and bounded communication volumes and enhanced balanced workload; on the architecture level, we consider a mixture of accelerators (i.e., sparse and dense accelerators) with a dedicated GCN training accelerator and a fine-grained pipeline; on the algorithm level, we propose a mixture of experts model based on the proposed system. Extensive experiments show that MIXGCN achieves boosted training efficiency, scalability, and better convergence as compared to the state-of-the-art methods. All codes will be released upon acceptance.</p>
<p><a id="user-content-clzy"><strong>Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue Wang, Yingyan (Celine) Lin - "HW-NAS-Bench: Hardware-aware Neural Architecture Search Benchmark"</strong></a></p>
<p>"HardWare-aware Neural Architecture Search (HW-NAS) has recently gained tremendous attention by automating the design of deep neural networks deployed in more resource-constrained daily life devices. Despite its promising performance, developing optimal HW-NAS solutions can be prohibitively challenging as it requires cross-disciplinary knowledge in the algorithm, micro-architecture, and device-specific compilation. First, to determine the hardware-cost to be incorporated into the NAS process, existing works mostly adopt either pre-collected hardware-cost look-up tables or device-specific hardware-cost models. The former can be time-consuming due to the required knowledge of the device’s compilation method and how to set up the measurement pipeline, while building the latter is often a barrier for non-hardware experts like NAS researchers. Both of them limit the development of HW-NAS innovations and impose a barrier-to-entry to non-hardware experts. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to their significant required computational resources and the differences in adopted search spaces, hyperparameters, and hardware devices. To this end, we develop HW-NAS-Bench, the first public dataset for HW-NAS research which aims to democratize HW-NAS research to non-hardware experts and make HW-NAS research more reproducible and accessible. To design HW-NAS-Bench, we carefully collected the measured/estimated hardware performance (e.g., energy cost and latency) of all the networks in the search spaces of both NAS-Bench-201 and FBNet, on six hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we provide a comprehensive analysis of the collected measurements in HW-NAS-Bench to provide insights for HW-NAS research. Finally, we demonstrate exemplary user cases to (1) show that HW-NAS-Bench allows non-hardware experts to perform HW-NAS by simply querying our pre-measured dataset and (2) verify that dedicated device-specific HW-NAS can indeed lead to optimal accuracy-cost trade-offs. The codes and all collected data are available at <a href="https://github.com/RICE-EIC/HW-NAS-Bench">https://github.com/RICE-EIC/HW-NAS-Bench</a>.
"</p>
<p><a id="user-content-pl"><strong>Patrick Lavin, Sudhanshu Agarwal, Ryan Lunch, Jeffrey Young, Richard Vuduc - "Multifidelity DRAM Simulation in SST"</strong></a></p>
<p>Simulation is slow. In all fields, we will always want to run faster simulations so that we can evaluate more design points or run more experiments. In the simulation of physical systems, researchers often use multifidelity models that evaluate different sections of the domain at different levels of detail; for instance, when simulating fluids, more detail is needed near eddies to fully capture the phenomena. Taking inspiration from such work, we present an online methodology for multifidelity computer architecture simulation and show how it can be used to accelerate DRAM simulations in the popular Structural Simulation Toolkit.</p>
<p><a id="user-content-ai"><strong>Afolabi Ige, Jennifer Hasler - "Synthesizing Analog Computing ASICs with Programmable Standard Cells"</strong></a></p>
<p>There has been a clear divide in tooling and scale for digital vs analog computing. This work presents an automated tool to synthesize analog systems to an Integrated Circuit (IC) utilizing a programmable analog cell library. Synthesis using analog cells requires awareness of Floating-Gate (FG) infrastructure and appropriately clustering these cells into "islands". This work presents an automated tool that takes a Verilog description and uses a novel architecture approach to coalesce related FG and non FG cells into an island, which can then be routed to other islands to form a large scale system. A PDK converter transforms GDSII cells between foundry processes. We demonstrate the capability of the tool by synthesizing an embedded speech classifier.</p>
<p><a id="user-content-mi"><strong>Mikhail Isaev, Nic McDonald, Jeff Young, Rich Vuduc - "ParaGraph: An application-simulator interface and toolkit for hardware-software co-design"</strong></a></p>
<p>We present ParaGraph, a novel infrastructure for realistic application representation and bridging with existing high-fidelity hardware simulators to model the performance of supercomputers.
ParaGraph aims to decouple application modeling from hardware modeling by bridging existing graph-based application representations and corresponding performance engineering tools with computer simulators typical in future hardware architecture research.
The first component of ParaGraph is an abstract graph representation of parallel programs suitable for interfacing with existing compilers for automatic workload extraction and modeling system software such as communication libraries.
The second component is a runtime that can emulate this representation's dynamic execution on backend simulators.
Together, these components create a slim middle-end interface between various graph-based program representations on the frontend and different computer system simulators on the backend.
This interface facilitates the productivity of researchers working in the different domains of hardware-software co-design. It lets the experts in each field be productive with their familiar tools while hiding the complexity of complementary field instruments and corresponding interfacing.
We have conducted case studies to show that one can use ParaGraph to flexibly build different modeling workflows.
These include an exploration of hardware and software configurations to optimize the performance of all-reduce;
TPU trace matching on MLPerf training benchmarks;
and running a deep learning application on a vendor-specific simulator, utilizing multiple frontend frameworks and backend simulators across tasks.
ParaGraph is freely available open-source software. In the future, we plan to extend this work to support more simulators and program representations.</p>
<p><a id="user-content-yf"><strong>Yonggan Fu, Yang Zhao, Yonggan Fu, Qixuan Yu, Yonggan Fu, Chaojian Li, Yonggan Fu, Yingyan (Celine) Lin - "2-in-1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency"</strong></a></p>
<p>The recent breakthroughs of deep neural networks (DNNs) and the advent of billions of Internet of Things (IoT) devices have excited an explosive demand for intelligent IoT devices equipped with domain-specific DNN accelerators. However, the deployment of DNN accelerator enabled intelligent functionality into real-world IoT devices still remains particularly challenging. First, powerful DNNs often come at prohibitive complexities, whereas IoT devices often suffer from stringent resource constraints. Second, while DNNs are vulnerable to adversarial attacks, especially on IoT devices exposed to complex real-world environments, many IoT applications require strict security. Existing DNN accelerators mostly tackle only one of the two aforementioned challenges (i.e., efficiency or adversarial robustness) while neglecting or even sacrificing the other. To this end, we propose a 2-in-1 Accelerator, an integrated algorithm-accelerator co-design framework aiming at winning both the adversarial robustness and efficiency of DNN accelerators. Specifically, we first propose a Random Precision Switch (RPS) algorithm that can effectively defend DNNs against adversarial attacks by enabling random DNN quantization as an in-situ model switch during training and inference. Furthermore, we propose a new precision-scalable accelerator featuring (1) a new precision-scalable MAC unit architecture that spatially tiles the temporal MAC units to boost both the achievable efficiency and flexibility and (2) a systematically optimized dataflow that is searched by our generic accelerator optimizer. Extensive experiments and ablation studies validate that our 2-in-1 Accelerator can not only aggressively boost both the adversarial robustness and efficiency of DNN accelerators under various attacks, but also naturally support instantaneous robustness-efficiency trade-offs adapting to varied resources without the necessity of DNN retraining. We believe our 2-in-1 Accelerator has opened up an exciting perspective for robust and efficient accelerator design.</p>
<p><a id="user-content-hy"><strong>Haoran You, Cheng Wan, Yang Zhao, Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Shang Wu, Shunyao Zhang, Yongan Zhang, Chaojian Li, Vivek Boominathan, Ashok Veeraraghavan, Ziyun Li, Yingyan Lin - "EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm &amp; Accelerator Co-Design"</strong></a></p>
<p>Eye tracking has become an essential human-machine interaction modality for providing immersive experience in numerous virtual and augmented reality (VR/AR) applications desiring high throughput (e.g., 240 FPS), small-form, and enhanced visual privacy. However, existing eye tracking systems are still limited by their: (1) large form-factor largely due to the adopted bulky lens-based cameras; and (2) high communication cost required between the camera and backend processor, thus prohibiting their more extensive applications. To this end, we propose a lensless FlatCam-based eye tracking algorithm and accelerator co-design framework dubbed EyeCoD to enable eye tracking systems with a much reduced form-factor and boosted system efficiency without sacrificing the tracking accuracy, paving the way for next-generation eye tracking solutions. On the system level, we advocate the use of lensless FlatCams to facilitate the small form-factor need in mobile eye tracking systems. On the algorithm level, EyeCoD integrates a predict-then-focus pipeline that first predicts the region-of-interest (ROI) via segmentation and then only focuses on the ROI parts to estimate gaze directions, greatly reducing redundant computations and data movements. On the hardware level, we further develop a dedicated accelerator that (1) integrates a novel workload orchestration between the aforementioned segmentation and gaze estimation models, (2) leverages intra-channel reuse opportunities for depth-wise layers, and (3) utilizes input feature-wise partition to save activation memory size. On-silicon measurement validates that our EyeCoD consistently reduces both the communication and computation costs, leading to an overall system speedup of 10.95x, 3.21x, and 12.85x over CPUs, GPUs, and a prior-art eye tracking processor called CIS-GEP, respectively, while maintaining the tracking accuracy.</p>
<p><a id="user-content-yz"><strong>Yongan Zhang, Haoran You, Yonggan Fu, Tong Geng, Ang Li, Yingyan Lin - "G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency"</strong></a></p>
<p>Graph Neural Networks (GNNs) have emerged as the state-of-the-art (SOTA) method for graph-based learning tasks. However, it still remains prohibitively challenging to inference GNNs over large graph datasets, limiting their application to large-scale real-world tasks. While end-to-end jointly optimizing GNNs and their accelerators is promising in boosting GNNs' inference efficiency and expediting the design process, it is still underexplored due to the vast and distinct design spaces of GNNs and their accelerators. In this work, we propose G-CoS, a GNN and accelerator co-search framework that can automatically search for matched GNN structures and accelerators to maximize both task accuracy and acceleration efficiency. Specifically, GCoS integrates two major enabling components: (1) a generic GNN accelerator search space which is applicable to various GNN structures and (2) a one-shot GNN and accelerator co-search algorithm that enables simultaneous and efficient search for optimal GNN structures and their matched accelerators. To the best of our knowledge, G-CoS is the first co-search framework for GNNs and their accelerators. Extensive experiments and ablation studies show that the GNNs and accelerators generated by G-CoS consistently outperform SOTA GNNs and GNN accelerators in terms of both task accuracy and hardware efficiency, while only requiring a few hours for the end-to-end generation of the best matched GNNs and their accelerators.</p>
<p><a id="user-content-fmm"><strong>Francisco Munoz Martinez, Raveesh Garg, José L. Abellán, Manuel E Acacio, Clay Hughes, Siva Rajamanickam, Tushar Krishna - "Cycle Accurate Simulation of AI Applications using STONNE, SST-STONNE and OMEGA"</strong></a></p>
<p>The design of specialized architectures for accelerating the Deep Learning inference is a booming area of research nowadays. While first-generation accelerators used simple systolic structures with fixed dataflows tailored for dense Deep Neural Networks applications, more recent architectures from startups such as SambaNova and Cerebras, and academia (such as MAERI and SIGMA) have argued for flexibility to efficiently support a wide variety of layer types, dimensions, and sparsity, by enabling dataflow execution. As the complexity of these accelerators grows, the analytical models currently being used for design-space exploration are unable to capture execution-time subtleties, leading to inexact results in many cases. This opens up a need for cycle-level simulation tools to allow for fast and accurate design-space exploration of DL accelerators, and rapid quantification of the efficacy of architectural enhancements during the early stages of a design. To this end, we propose STONNE (Simulation TOol of Neural Network Engines) which is a cycle-level microarchitectural simulation framework that can plug into any high-level DL framework as an accelerator device and perform full-model evaluation of state-of-the-art systolic and flexible DNN accelerators We also propose various enhancements to the core STONNE simulator - (1) OMEGA: A simulator for Graph Neural Network Dataflows (2) SST-STONNE: Integration of STONNE as an element of Structural Simulation Toolkit (SST).</p>
<p><a id="user-content-yz"><em><em>Albert Cho</em>, Anish Saxena</em>, Srikar Vanavasam - "Use CXL, not DDR, for Scalable Server Processors"**</a></p>
<p>"The memory system is a major performance determinant for server processors. Ever-growing core counts and datasets demand higher bandwidth and capacity as well as lower latency from the memory system. However, because DDR interface ---the dominant interface to memory over the past two decades--- requires a large number of on-chip pins, the processor's memory bandwidth is ultimately restrained by its pin-count, a scarce resource. With limited bandwidth, memory requests contend for each memory channel, resulting in significant queuing delays that overshadow DRAM's service time and degrade performance.
To that end, we present CoaXiaL, a server design that affords much higher bandwidth by replacing all DDR interfaces to the processor with the more pin-efficient CXL interface. CXL offers 4X higher bandwidth per pin compared to DDR at a modest latency overhead. We first demonstrate CXL's latency premium is more than offset by its higher bandwidth. As CoaXiaL distributes memory requests across 4X more channels, it reduces queuing delays and thereby memory access latency. Second, we show that even with similar average memory access time, variance in its latency distribution significantly degrades performance. CoaXiaL reduces memory latency variance with its high bandwidth, further boosting performance. Our experiments show that such a CXL-centric system improves the performance of server workloads by 1.5X on average and by up to 3.1X."</p>
<p><a id="user-content-rs"><strong>Rishov Sarkar, Cong (Callie) Hao - "LightningSim: Fast and Accurate Trace-Based Simulation for HLS"</strong></a></p>
<p>High-Level Synthesis allows hardware designers to create complex RTL designs using C/C++. The traditional HLS workflow involves iterations of C/C++ simulation for partial functional verification and HLS synthesis for coarse timing estimates. However, neither C/C++ simulation nor HLS synthesis estimates can account for complex behaviors like FIFO interactions and pipeline stalls, thereby obscuring problems like deadlocks and latency overheads. Such problems are revealed only through C/RTL co-simulation, which is typically orders of magnitude slower than either C/C++ simulation or HLS synthesis, far too slow to integrate into the edit-run development cycle. Addressing this, we propose LightningSim, a fast simulation tool for HLS that combines the speed of native C/C++ with the accuracy of C/RTL co-simulation. LightningSim directly operates on the LLVM intermediate representation (IR) code and accurately simulates hardware design’s dynamic behavior. First, it traces LLVM IR execution to capture the run-time information; second, it maps the static HLS scheduling information to the trace to simulate the dynamic behavior; third, it calculates stalls and deadlocks from inter-function interactions to get precise cycle counts. Evaluated on 33 benchmarks, LightningSim produces 99.9%-accurate timing estimates up to 95× faster than RTL simulation.</p>
<p><a id="user-content-zjs"><strong>Zhixin (Jack) Song, Bryan Gard, Spencer Bryngelson - "Solving Partial Differential Equations on Noisy Quantum Computers"</strong></a></p>
<p>Numerical solutions to partial differential equations (PDEs) is ab initio important for scientific research and engineering design. Quantum algorithms have been proposed to solve PDEs and verified on small-scale simulators. However, they ignore the effect of quantum hardware noise, which is critical for solving such problems on current and near-term quantum hardware. The utility of current and future quantum algorithms for PDEs is left open by this knowledge gap. We propose an effort that uses quantum noise models to assess the viability of different classes of quantum PDE solvers.</p>
<p><a id="user-content-pgcw"><strong>Pulkit Gupta, Cynthia Wang, Shunzhi Wen, Tom Conte - "A High ILP Architecture for Supercunducting Electronics (SCE)"</strong></a></p>
<p>"Superconducting Electronics (SCE) offer opportunities to reach the minimum energy required for computation. That said, SCE and a majority of superconducting logic families face difficult tradeoffs and constraints that require creativity to overcome. Due to constraints on memory size and minimal support for highly ported structures, the majority of existing high-ILP (Instruction Level Parallelism) architectures are not compatible with SCE. We propose a new architecture, the Static sYNchronous Dataflow Risc Architecture (SYNDRA). SYNDRA aims to combine the benefits of statically scheduled VLIW architectures with the register-free execution model of dataflow architectures.</p>
<p>Superscalar processors struggle to find ILP when load latencies are high unless the scheduling window is substantially large. Additionally, scheduling queues dynamically build data dependence graphs by using register renaming to eliminate anti/output dependences. Using static scheduling techniques from VLIW such as list/operation scheduling and treegion scheduling, allows us to limit the computational effort and energy expenditure of large scheduling queues by precomputing the schedule in the compiler. In VLIW processors, the register file serves as a centralized indexable structure that enforces safe execution of a linear stream of wide instructions. SYNDRA combines the energy cost reduction of VLIW/static instruction scheduling with a statically determined dataflow schedule/graph to eliminate a large amount of on-chip effort and register file demands by using compiler driven statically exposed ILP."</p>
<p><a id="user-content-srww"><strong>Saeed Rashidi, William Won, Sudarshan Srinivasan, Srinivas Sridharan, Tushar Krishna - "Themis: A Network Bandwidth-Aware Collective Scheduling Policy for Distributed Training of DL Models"</strong></a></p>
<p>"Distributed training is a solution to reduce DNN training time by splitting the task across multiple NPUs (e.g., GPU/TPU). However, distributed training adds communication overhead between the NPUs in order to synchronize the gradients and/or activation, depending on the parallelization strategy. In next-generation platforms for training at scale, NPUs will be connected through multi-dimensional networks with diverse, heterogeneous bandwidths.
This work identifies a looming challenge of keeping all network dimensions busy and maximizing the network BW within the hybrid environment if we leverage scheduling techniques for collective communication on systems today. We propose Themis, a novel collective scheduling scheme that dynamically schedules collectives (divided into chunks) to balance the communication loads across all dimensions, further improving the network BW utilization. Our results show that on average, Themis can improve the network BW utilization of the single All-Reduce by 1.72x (2.70x max), and improve the end-to-end training iteration performance of real workloads such as ResNet-152, GNMT, DLRM, and Transformer-1T by 1.49x (2.25x max), 1.30x (1.78x max), 1.30x (1.77x max), and 1.25x (1.53x max), respectively."</p>
<p><a id="user-content-vgaa"><strong>Vima Gupta, Austin Adams, Elton Pinto, Dr. Jeffrey Young, Dr. Tom M Conte - "Effective qubit mapping, routing and scheduling for Ion-Shuttling Quantum Architectures"</strong></a></p>
<p>"Trapped-Ion Linear Tape (TILT) architectures offer a scalable way to realize ion-trapped quantum computers through tape-based shuttling and routing operations. Modulo a cost model for tape movement and gate application, the quality of qubit mapping and routing (QMR) targeting TILT architectures has a tangible impact on circuit fidelity. State-of-the-art QMR techniques either account for the cost of tape movement or rely on heuristic-based approaches.
In this work, we introduce and evaluate MALT, a comprehensive extension of MaxSAT-based approach. To address the shortcomings of existing qubit mapping and routing (QMR) techniques, we introduce and evaluate MALT, a comprehensive extension of a MaxSAT based QMR technique by Molavi et. al. MALT generates efficient swap insertion and tape movement schedule for shuttling based architectures, geared towards improving circuit fidelity. We are working on addressing the issues in scaling a pure constraint based approach and showcase the performance of a hybrid technique."</p>
<p><a id="user-content-rlaa"><strong>Ryan Lynch, Austin Adams, Tom Conte, Jeff Young - "Leveraging MLIR to Augment a Python Quantum DSL"</strong></a></p>
<p>We present a proposed and in-progress technique for using LLVM's MLIR infrastructure to augment Qiskit, a Python DSL for quantum programming, by improving dataflow analyses and ease of optimization.</p>
<p><a id="user-content-ajpg"><strong>Anirudh Jain, Pulkit Gupta, Tom Conte - "Residue Matrices for accelerating Sparse Kernels &amp; GraphBLAS"</strong></a></p>
<p>While tiling for dense matrix kernels is well understood and universally applied, sparse kernels have yet to receive the same treatment. The irregular sparsity and distribution of non-zeroes in sparse matrices make a 'one size fits all' approach to tiling ineffective. This work proposed to construct a lightweight signature for sparse inputs and perform operations on it to make a principled decision on tile size and kernel loop ordering without having to sweep the search space. We demonstrate a 1.43x speedup over the current state of the art for the SpMM kernel.</p>
<p><a id="user-content-rlaa"><strong>Payman Behnam, Jianming Tong, Alind Khare, Tushar Krishna, Alexey Tumanov - "SUSHI: SubGraph Stationary HW-SW Co-design for ML Inference"</strong></a></p>
<p>"A growing number of applications depends on machine learning functionality and benefits from both higher quality of ML predictions (accuracy) and better timeliness (latency) at the same time. Existing work that focuses on reaching better latency/accuracy yields improvements for a single point in the latency/accuracy tradeoff space.
We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that activates different SubNets within this weight-shared construct. It creates an opportunity to exploit the inherent temporal locality with our proposed SubGraph Stationary optimization. We take a hardware-software co-design approach with a real implementation of SubGraph Stationary in a novel FPGA implementation of a software
scheduler controlling which SubNets to serve and what to cache in real-time. An Abstraction is proposed to decouple the scheduler from the underlying hardware, i.e., the change in the hardware should not require any changes in the scheduler policy code. Combined, they are vertically integrated into SUSHI—an inference serving stack, which yields up to 41% improvement in latency, 1% increase in served accuracy, and achieves up to 52.6% saved energy."</p>
<p><a id="user-content-rlaa"><strong>Blaise Tine, Ruobing Han, Fares Elsabbagh, Krishna Praveen, Apurve Chawda, Will Gulian, Yaotian Feng, Da Eun Shim, Priyadarshini Roshan, Ethan Lyons, Varun Saxena, Santosh Srivatsan, Joshua R. Simpson, Fadi Alzammar, Liam Cooper, Sam Jijina, Swetha Rajagoplan, Tejaswini Anand Kumar, Jeff Young, Hyesoon Kim - "Vortex: Open GPU Research Platform"</strong></a></p>
<p>Vortex is an open-source hardware and software project to support GPU using RISC-V ISA extensions. Vortex supports OpenCL and runs on FPGA. Vortex has also been extended to accelerate graphics rendering with Vulkan. The Vortex platform is extensible and scalable, including a completely open-source compiler, driver, and runtime software stack, which enables end-to-end research in the GPU domain.</p>
</article></body></html>